# Assignment-2-Data-Bias

In this assignment, we were provided a raw csv file consisting of thousands of Wikipedia comments. For each comment, there is an identifier and a toxicity score generated by the Perspective API between 0 and 1. There are also 6 comment 'labels' or categories, and these correspond to scores given by human reviewers. The labels are 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', and 'identity_hate'. For each comment, the human reviewers gave a score of 0 or 1 for each label respective to each comment. A comment with a 1 in the obscene column and a 0 in the severe_toxic column would indicate a comment with an obscenity but not severely toxic. 

After manually inspecting the set of comments, I found that comments with identity terms were especially common when being marked as toxic or severely toxic. This led me to hypothesize that the Perspective API was prone to dealing out false positives for toxicity for comments that contained identity terms. I ran a couple tests on the DataFrame consisting all comments and scores to see what corresponds to a severely toxic or identity hate comment. Through the tests and manual inspection of comments, I settled on 0.5 as my threshold. This threshold served as a comparison for my own tests that I ran. A score greater than the threshold of 0.5 would indicate a toxic comment, and vice versa for a score less than 0.5.

To test my initial hypothesis that identity term comments would get false positives, I created a text file consisting of 12 comments that contained identity terms ('Black, 'Muslim', 'Women', 'Jews') in positive comments. An example of a comment is 'Muslims have a very unique culture and get too much hate'. This comment contains the identity term of 'Muslim' but is depicted in a positive manner. This comment scored 0.40 by the perspective API, which is less than the threshold, but still a little high for a positive comment. Not a single one of my 12 made up positive identity comments was scored above 0.5 by the Perspective API, thus disproving my hypothesis. 

Out of the mini 12 comment dataset, the average toxicity score was 0.26, and not a single comment was scored over the threshold. The low sample size of 12 comments makes it difficult to say with certainty that the Perspective API does not give false positives to identity comments, as I feel if a larger sample size with hundreds or thousands of comments would show a trend of occasional false positives with identity comments. The reason I hypothesized that identity comments would be flagged with false positives is that many toxic comments and hate comments online involve identity terms as the comments can be aimed at a certain group. This led me to believe that the ML model would correlate identity comments with higher toxicity scores due to the widespread number of hateful identity comments. 

This exercise and assignment was very insightful in how teams develop machine learning models and tools to create a safer environment online and elsewhere. It also showcases how important and rigorous testing can be, as with ML models, you need to keep testing to ensure reliability. I started to think about the Perspective API, and what the steps would be for ensuring the reliability of the model. The internet and social media is on such a large scale, with millions of comments and posts going out everyday. How did Google's engineers test the model to ensure that it would be reliable over such a large scale? Another question I had was regarding bias. What strategies can engineers and developers take to mitigate bias when it is first discovered in whatever model you are building/testing?
